
<img src="https://github.com/Yongli-Lisa/Linux-Notes1/blob/f11944d00248a003d8df31a8a62eb55417628128/Img/%E8%99%9A%E6%8B%9F%E5%8C%96/%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%961.PNG" width="400px">  

  
<img src="https://github.com/Yongli-Lisa/Linux-Notes1/blob/f11944d00248a003d8df31a8a62eb55417628128/Img/%E8%99%9A%E6%8B%9F%E5%8C%96/%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%962.PNG" width="1000px">  



virtio: 虚拟化 I/O 设备  
virtio 负责对于虚拟机提供统一的接口。也就是说，在虚拟机里面的操作系统加载的驱动，以后都统一加载 virtio 就可以了   

Virtio架构：  
<img src="https://github.com/Yongli-Lisa/Linux-Notes1/blob/f11944d00248a003d8df31a8a62eb55417628128/Img/%E8%99%9A%E6%8B%9F%E5%8C%96/Virtio%E6%9E%B6%E6%9E%84.PNG" width="300px">  
<img src="https://github.com/Yongli-Lisa/Linux-Notes1/blob/f11944d00248a003d8df31a8a62eb55417628128/Img/%E8%99%9A%E6%8B%9F%E5%8C%96/Virtio%E5%9B%9B%E5%B1%82%E6%9E%B6%E6%9E%84.PNG" width="300px">    

首先，在虚拟机里面的 virtio 前端，针对不同类型的设备有不同的驱动程序，但是接口都是统一的。例如，硬盘就是 virtio_blk，网络就是 virtio_net。  
其次，在宿主机的 qemu 里面，实现 virtio 后端的逻辑，主要就是操作硬件的设备。例如通过写一个物理机硬盘上的文件来完成虚拟机写入硬盘的操作。再如向内核协议栈发送一个网络包完成虚拟机对于网络的操作。  
在 virtio 的前端和后端之间，有一个通信层，里面包含 virtio 层和 virtio-ring 层。virtio 这一层实现的是虚拟队列接口，算是前后端通信的桥梁。而 virtio-ring 则是该桥梁的具体实现。  
virtio 使用 virtqueue 进行前端和后端的高速通信。不同类型的设备队列数目不同。virtio-net 使用两个队列，一个用于接收，另一个用于发送；而 virtio-blk 仅使用一个队列。  
如果客户机要向宿主机发送数据，客户机会将数据的 buffer 添加到 virtqueue 中，然后通过写入寄存器通知宿主机。这样宿主机就可以从 virtqueue 中收到的 buffer 里面的数据。  


存储虚拟化架构：  
<img src="https://github.com/Yongli-Lisa/Linux-Notes1/blob/f11944d00248a003d8df31a8a62eb55417628128/Img/%E8%99%9A%E6%8B%9F%E5%8C%96/%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%87%E7%A8%8B.PNG" width="600px">    
前端有前端的块设备驱动 Front-end driver，在客户机的内核里面，它符合普通设备驱动的格式，对外通过 VFS 暴露文件系统接口给客户机里面的应用。  
后端有后端的设备驱动 Back-end driver，在宿主机的 qemu 进程中，当收到客户机的写入请求的时候，调用文件系统的 write 函数，写入宿主机的 VFS 文件系统，最终写到物理硬盘设备上的 qcow2 文件。  
中间的队列用于前端和后端之间传输数据，在前端的设备驱动和后端的设备驱动，都有类似的数据结构 virt-queue 来管理这些队列。  



存储虚拟化写入过程：  
<img src="https://github.com/Yongli-Lisa/Linux-Notes1/blob/f11944d00248a003d8df31a8a62eb55417628128/Img/%E8%99%9A%E6%8B%9F%E5%8C%96/%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E5%86%99%E5%85%A5%E8%BF%87%E7%A8%8B.PNG" width="1600px">    
在虚拟机里面，应用层调用 write 系统调用写入文件。  
write 系统调用进入虚拟机里面的内核，经过 VFS，通用块设备层，I/O 调度层，到达块设备驱动。  
虚拟机里面的块设备驱动是 virtio_blk，它和通用的块设备驱动一样，有一个 request queue，另外有一个函数 make_request_fn 会被设置为 blk_mq_make_request，这个函数用于将请求放入队列。  
虚拟机里面的块设备驱动是 virtio_blk 会注册一个中断处理函数 vp_interrupt。当 qemu 写入完成之后，它会通知虚拟机里面的块设备驱动。  
blk_mq_make_request 最终调用 virtqueue_add，将请求添加到传输队列 virtqueue 中，然后调用 virtqueue_notify 通知 qemu。  
在 qemu 中，本来虚拟机正处于 KVM_RUN 的状态，也即处于客户机状态。  
qemu 收到通知后，通过 VM exit 指令退出客户机状态，进入宿主机状态，根据退出原因，得知有 I/O 需要处理。  
qemu 调用 virtio_blk_handle_output，最终调用 virtio_blk_handle_vq。  
virtio_blk_handle_vq 里面有一个循环，在循环中，virtio_blk_get_request 函数从传输队列中拿出请求，然后调用 virtio_blk_handle_request 处理请求。  
virtio_blk_handle_request 会调用 blk_aio_pwritev，通过 BlockBackend 驱动写入 qcow2 文件。  
写入完毕之后，virtio_blk_req_complete 会调用 virtio_notify 通知虚拟机里面的驱动。数据写入完成，刚才注册的中断处理函数 vp_interrupt 会收到这个通知。



